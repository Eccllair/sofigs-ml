{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa86d0cc",
   "metadata": {},
   "source": [
    "# Файл с общим анализом данных и фрагментами функционала"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a017e",
   "metadata": {},
   "source": [
    "### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb15dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "categoryes = pd.read_csv(\"training-data/retail-rocket/category_tree.csv\")\n",
    "events = pd.read_csv(\"training-data/retail-rocket/events.csv\")\n",
    "properties1 = pd.read_csv(\"training-data/retail-rocket/item_properties_part1.csv\")\n",
    "properties2 = pd.read_csv(\"training-data/retail-rocket/item_properties_part2.csv\")\n",
    "\n",
    "events = events.sort_values(by=\"timestamp\")\n",
    "properties = pd.concat([properties1, properties2])\n",
    "\n",
    "#data truncation\n",
    "# categoryes = categoryes[0:1000]\n",
    "# events = events[0:1000]\n",
    "# properties = properties1[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e4387",
   "metadata": {},
   "source": [
    "### Обрабатываем категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6473cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoryid    int64\n",
      "parentid      int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoryid</th>\n",
       "      <th>parentid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>1452</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1182</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1490</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>791</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>140</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      categoryid  parentid\n",
       "1476        1452        -1\n",
       "1484        1182        -1\n",
       "861         1490        -1\n",
       "589          791        -1\n",
       "1629         140        -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NONE = -1\n",
    "\n",
    "sorted_categoryes = categoryes.fillna(NONE).sort_values(by=\"parentid\")\n",
    "sorted_categoryes[\"parentid\"] = sorted_categoryes[\"parentid\"].astype(int)\n",
    "print(sorted_categoryes.dtypes)\n",
    "sorted_categoryes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c7b34",
   "metadata": {},
   "source": [
    "#### Представляем категории в виде дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0738881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.modules.categories import Tree, TreeNode\n",
    "\n",
    "\n",
    "# заполняем словарь parent - childrens\n",
    "category_map: dict[int, list[int]] = {}\n",
    "for _, row in sorted_categoryes.iterrows():\n",
    "    parentid = int(row['parentid'])\n",
    "    categoryid = int(row['categoryid'])\n",
    "    \n",
    "    if parentid not in category_map:\n",
    "        category_map[parentid] = []\n",
    "    category_map[parentid].append(categoryid)\n",
    "\n",
    "# заполняем дерево\n",
    "categoryes_tree = Tree()\n",
    "\n",
    "stack: list[TreeNode] = [categoryes_tree._root]\n",
    "while len(stack) > 0:\n",
    "    node = stack.pop()\n",
    "    \n",
    "    key = node.value if node.value != \"\" else NONE\n",
    "    if key not in category_map:\n",
    "        continue\n",
    "        \n",
    "    for child_value in category_map[key]:\n",
    "        stack.append(categoryes_tree.append_to(node, child_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryes_tree._root.childrens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d30d11",
   "metadata": {},
   "source": [
    "### Обрабатывем события"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6037e6e",
   "metadata": {},
   "source": [
    "Находим уникальные события, чтобы создать EventEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6f8978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['addtocart', 'view', 'transaction'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[\"event\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb9de8",
   "metadata": {},
   "source": [
    "находим последовательность действий для всех пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec846e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from app.common.schemas import Event\n",
    "from app.common.enums import EventEnum\n",
    "\n",
    "\n",
    "users_events: dict[int, list[Event]] = {}\n",
    "for _, user_event in events.iterrows():\n",
    "    pyd_event = Event(\n",
    "        event=EventEnum(user_event[\"event\"]),\n",
    "        user_id=user_event[\"visitorid\"],\n",
    "        item_id=user_event[\"itemid\"],\n",
    "        transaction_id=user_event[\"transactionid\"] if not np.isnan(user_event[\"transactionid\"]) else None,\n",
    "        timestamp=user_event[\"timestamp\"]\n",
    "    )\n",
    "    if users_events.get(user_event[\"visitorid\"]) is None:\n",
    "        users_events[user_event[\"visitorid\"]] = [pyd_event]\n",
    "    else:\n",
    "        users_events[user_event[\"visitorid\"]].append(pyd_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12ef54",
   "metadata": {},
   "source": [
    "### Обрабатывем параметры предметов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c2c5b",
   "metadata": {},
   "source": [
    "```python\n",
    "formalizable_properties: dict[\n",
    "    int,  # property\n",
    "    int   # индекс формализуемости: число_использований/число_всех_значений_параметра\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121ea2e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m         stat_properties[item_property[\u001b[33m\"\u001b[39m\u001b[33mproperty\u001b[39m\u001b[33m\"\u001b[39m]][\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         stat_properties[\u001b[43mitem_property\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproperty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m][\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     17\u001b[39m         stat_properties[item_property[\u001b[33m\"\u001b[39m\u001b[33mproperty\u001b[39m\u001b[33m\"\u001b[39m]][\u001b[32m1\u001b[39m].append(item_property[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# находим наиболее формализуемые параметры\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1108\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1109\u001b[39m     key = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:2801\u001b[39m, in \u001b[36mcheck_dict_or_set_indexers\u001b[39m\u001b[34m(key)\u001b[39m\n\u001b[32m   2789\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2790\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m   2791\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m   2792\u001b[39m \u001b[33;03m    bool\u001b[39;00m\n\u001b[32m   2793\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   2795\u001b[39m         obj.start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2796\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m obj.stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2797\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (obj.step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m obj.step != \u001b[32m1\u001b[39m)\n\u001b[32m   2798\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_dict_or_set_indexers\u001b[39m(key) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2802\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2803\u001b[39m \u001b[33;03m    Check if the indexer is or contains a dict or set, which is no longer allowed.\u001b[39;00m\n\u001b[32m   2804\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2805\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2806\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mset\u001b[39m)\n\u001b[32m   2807\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[32m   2808\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[32m   2809\u001b[39m     ):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# удаляем параметры присущие только 5-ти процентам предметов\n",
    "properties_counts = properties[\"property\"].value_counts()\n",
    "properties_counts = properties_counts[properties_counts > int(len(properties_counts)*0.9)]\n",
    "properties_counts = properties_counts.index.tolist()\n",
    "\n",
    "# создаём таблицу формализуемости параметров\n",
    "stat_properties: dict[int, list[int, list[int]]] = {}\n",
    "for _, item_property in properties.iterrows():\n",
    "    if item_property[\"property\"] not in properties_counts:\n",
    "        continue\n",
    "    if stat_properties.get(item_property[\"property\"]) is None:\n",
    "        stat_properties[item_property[\"property\"]] = [1, [item_property[\"value\"]]]\n",
    "    elif item_property[\"value\"] in stat_properties[item_property[\"property\"]][1]:\n",
    "        stat_properties[item_property[\"property\"]][0] += 1\n",
    "    else:\n",
    "        stat_properties[item_property[\"property\"]][0] += 1\n",
    "        stat_properties[item_property[\"property\"]][1].append(item_property[\"value\"])\n",
    "\n",
    "# находим наиболее формализуемые параметры\n",
    "formalizable_properties = {prop: stat[0]/len(stat[1]) for prop, stat in stat_properties.items()}\n",
    "\n",
    "# разделяем параметры по популярности\n",
    "popular_properties = properties_counts[0:int(len(properties_counts)/3)]\n",
    "standart_properties = properties_counts[int(len(properties_counts)/3):int(len(properties_counts)*2/3)]\n",
    "unpopular_properties = properties_counts[int(len(properties_counts)*2/3):int(len(properties_counts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470ff6a",
   "metadata": {},
   "source": [
    "сохраняем полученные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b37aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "with Path(\"training-data/combined/popular_properties.json\").open(\"w\") as f:\n",
    "    json.dump(popular_properties, f)\n",
    "with Path(\"training-data/combined/standart_properties.json\").open(\"w\") as f:\n",
    "    json.dump(standart_properties, f)\n",
    "with Path(\"training-data/combined/unpopular_properties.json\").open(\"w\") as f:\n",
    "    json.dump(unpopular_properties, f)\n",
    "with Path(\"training-data/combined/formalizable_properties.json\").open(\"w\") as f:\n",
    "    json.dump(formalizable_properties, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b30f78",
   "metadata": {},
   "source": [
    "создаём таблицу со всеми снимками срстояний параметров для каждого параметра\n",
    "```python\n",
    "item_properties: dict[\n",
    "    int, # itemid\n",
    "    dict[ # все property для itemid\n",
    "        int, # property\n",
    "        list[ # список всех состояний для property\n",
    "            tuple[\n",
    "                int, # timestamp\n",
    "                str  # value\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b314d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_properties: dict[int, dict[int, list[tuple[int, str]]]] = {}\n",
    "item_avalable: dict[int, list[tuple[int, bool]]] = {}\n",
    "item_category: dict[int, list[tuple[int, int]]] = {}\n",
    "for _, values in properties.iterrows():\n",
    "    if values[\"property\"] == \"categoryid\":\n",
    "        if item_category.get(values[\"itemid\"]) is None:\n",
    "            item_category[values[\"itemid\"]] = [(values[\"timestamp\"], values[\"value\"])]\n",
    "        else:\n",
    "            item_category[values[\"itemid\"]].append((values[\"timestamp\"], values[\"value\"]))\n",
    "        continue\n",
    "\n",
    "    if values[\"property\"] == \"available\":\n",
    "        if item_avalable.get(values[\"itemid\"]) is None:\n",
    "            item_avalable[values[\"itemid\"]] = [(values[\"timestamp\"], values[\"value\"])]\n",
    "        else:\n",
    "            item_avalable[values[\"itemid\"]].append((values[\"timestamp\"], values[\"value\"]))\n",
    "        continue\n",
    "\n",
    "    if item_properties.get(values[\"itemid\"]) is None:\n",
    "        item_properties[values[\"itemid\"]] = {\n",
    "            values[\"property\"]: [(values[\"timestamp\"], values[\"value\"])]\n",
    "        }\n",
    "    elif item_properties[values[\"itemid\"]].get(values[\"property\"]) is None:\n",
    "        item_properties[values[\"itemid\"]][values[\"property\"]] = [(values[\"timestamp\"], values[\"value\"])]\n",
    "    else:\n",
    "        item_properties[values[\"itemid\"]][values[\"property\"]].append((values[\"timestamp\"], values[\"value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26cc9a",
   "metadata": {},
   "source": [
    "сохраняем полученные результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f53887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101ff48b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_properties' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Path(\u001b[33m\"\u001b[39m\u001b[33mtraining-data/combined/item_properties.json\u001b[39m\u001b[33m\"\u001b[39m).open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     json.dump(\u001b[43mitem_properties\u001b[49m, f)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Path(\u001b[33m\"\u001b[39m\u001b[33mtraining-data/combined/item_avalable.json\u001b[39m\u001b[33m\"\u001b[39m).open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     json.dump(item_avalable, f)\n",
      "\u001b[31mNameError\u001b[39m: name 'item_properties' is not defined"
     ]
    }
   ],
   "source": [
    "with Path(\"training-data/combined/item_properties.json\").open(\"w\") as f:\n",
    "    json.dump(item_properties, f)\n",
    "with Path(\"training-data/combined/item_avalable.json\").open(\"w\") as f:\n",
    "    json.dump(item_avalable, f)\n",
    "with Path(\"training-data/combined/item_category.json\").open(\"w\") as f:\n",
    "    json.dump(item_category, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = {\n",
    "    key: [\n",
    "        {\n",
    "            'event': str(value.event),\n",
    "            'user_id': value.user_id,\n",
    "            'item_id': value.item_id,\n",
    "            'transaction_id': value.transaction_id,\n",
    "            'timestamp': value.timestamp.timestamp()\n",
    "        } for value in values\n",
    "    ] for key, values in users_events.items()\n",
    "}\n",
    "with Path(\"training-data/combined/users_events.json\").open(\"w\") as f:\n",
    "    json.dump(f1, f)\n",
    "short_users_events:dict[int, list[dict[str, Any]]]=dict(itertools.islice(f1.items(), int(len(f1)/500)))\n",
    "with Path(\"training-data/combined/short_users_events.json\").open(\"w\") as f:\n",
    "    json.dump(short_users_events, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"training-data/combined/item_properties.json\").open(\"r\") as f:\n",
    "    item_properties=dict(json.loads(f.read()))\n",
    "with Path(\"training-data/combined/formalizable_properties.json\").open(\"r\") as f:\n",
    "    formalizable_properties=dict(json.loads(f.read()))\n",
    "with Path(\"training-data/combined/item_category.json\").open(\"r\") as f:\n",
    "    item_category=dict(json.loads(f.read()))\n",
    "with Path(\"training-data/combined/short_users_events.json\").open(\"r\") as f:\n",
    "    short_users_events=dict(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_item_properties: dict[int, dict[int, list[tuple[int, str]]]] = {}\n",
    "short_formalizable_properties: dict[int, float] = {}\n",
    "short_item_category: dict[int, list[tuple[int, int]]] = {}\n",
    "for events in short_users_events.values():\n",
    "    for event in events:\n",
    "        if short_item_properties.get(event[\"item_id\"]) is None and item_properties.get(str(event[\"item_id\"])) is not None:\n",
    "            short_item_properties[event[\"item_id\"]] = item_properties[str(event[\"item_id\"])]\n",
    "        if short_item_category.get(event[\"item_id\"]) is None and item_category.get(str(event[\"item_id\"])) is not None:\n",
    "            short_item_category[event[\"item_id\"]] = item_category[str(event[\"item_id\"])]\n",
    "for props in short_item_properties.values():\n",
    "    for prop in props.keys():\n",
    "        if short_formalizable_properties.get(prop) is None and formalizable_properties.get(str(prop)) is not None:\n",
    "            short_formalizable_properties[prop] = formalizable_properties[str(prop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"training-data/combined/short_item_properties.json\").open(\"w\") as f:\n",
    "    json.dump(short_item_properties, f)\n",
    "with Path(\"training-data/combined/short_item_category.json\").open(\"w\") as f:\n",
    "    json.dump(short_item_category, f) \n",
    "with Path(\"training-data/combined/short_formalizable_properties.json\").open(\"w\") as f:\n",
    "    json.dump(short_formalizable_properties, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8739ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_item_category: dict[int, list[tuple[int, int]]] = {}\n",
    "for events in short_users_events.values():\n",
    "    for event in events:\n",
    "        if (\n",
    "            short_item_category.get(event[\"item_id\"]) is None\n",
    "            and short_item_category.get(event[\"item_id\"]) is not None\n",
    "            and formalizable_properties.get(event[\"item_id\"]) is not None\n",
    "        ):\n",
    "            short_item_category[event[\"item_id\"]] = item_category[event[\"item_id\"]]\n",
    "with Path(\"training-data/combined/short_item_category.json\").open(\"w\") as f:\n",
    "    json.dump(short_item_category, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1dbd0",
   "metadata": {},
   "source": [
    "Используем сохраненные данные, очищая ОЗУ от предыдущих значений (нужно перезапустить ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e680d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from app.common.schemas import Event\n",
    "\n",
    "with Path(\"training-data/combined/short_users_events.json\").open(\"r\") as f:\n",
    "    short_users_events=dict(json.loads(f.read()))\n",
    "users_events: dict[int, list[Event]] = {\n",
    "    int(visitor_id): [\n",
    "        Event.model_validate(event) for event in user_events\n",
    "    ] for visitor_id, user_events in short_users_events.items()\n",
    "}\n",
    "\n",
    "with Path(\"training-data/combined/short_item_properties.json\").open(\"r\") as f:\n",
    "    item_properties=dict(json.loads(f.read()))\n",
    "with Path(\"training-data/combined/short_item_category.json\").open(\"r\") as f:\n",
    "    item_category=dict(json.loads(f.read()))\n",
    "with Path(\"training-data/combined/short_formalizable_properties.json\").open(\"r\") as f:\n",
    "    short_formalizable_properties=dict(json.loads(f.read()))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8734461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1407580"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f170df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.common.schemas import TrainingEvent\n",
    "from app.common.enums import SeasonEnum\n",
    "\n",
    "training_events: list[list[TrainingEvent]] =[]\n",
    "for user_events in users_events.values():\n",
    "    prev_event: Event | None = None\n",
    "    training_user_events = []\n",
    "    for user_event in sorted(user_events, key=lambda event: event.timestamp):\n",
    "        training_event = TrainingEvent(\n",
    "            event=user_event.event,\n",
    "            season=SeasonEnum((user_event.timestamp.month % 12) // 3),\n",
    "            timedelta_days=(user_event.timestamp - prev_event.timestamp).days if prev_event is not None else 0,\n",
    "        )\n",
    "        \n",
    "        actual_timestamp: int = 0\n",
    "        actual_category: int = 0\n",
    "        if item_category.get(str(user_event.item_id)) is not None:\n",
    "            for timestamp, category in item_category[str(user_event.item_id)]:\n",
    "                if user_event.timestamp.timestamp() - timestamp < user_event.timestamp.timestamp() - actual_timestamp and user_event.timestamp.timestamp() - timestamp >= 0:\n",
    "                    actual_timestamp = timestamp\n",
    "                    actual_category = category\n",
    "            training_event.item_category = actual_category\n",
    "        \n",
    "        form_factor = 0\n",
    "        if item_properties.get(str(user_event.item_id)) is not None:\n",
    "            for item_property, property_values in item_properties[str(user_event.item_id)].items():\n",
    "                if short_formalizable_properties.get(item_property) is not None and short_formalizable_properties[item_property] > form_factor:\n",
    "                    actual_timestamp: int = 0\n",
    "                    actual_value: str = \"\"\n",
    "                    for timestamp, values in property_values:\n",
    "                        if user_event.timestamp.timestamp() - timestamp/1000 < user_event.timestamp.timestamp() - actual_timestamp and user_event.timestamp.timestamp() - timestamp/1000 >= 0:\n",
    "                            actual_timestamp = int(timestamp/1000)\n",
    "                            actual_value = values\n",
    "                    if actual_value != \"\":\n",
    "                        training_event.item_property = item_property\n",
    "                        training_event.property_value = actual_value\n",
    "                        form_factor = short_formalizable_properties[item_property]\n",
    "        \n",
    "        training_user_events.append(training_event)\n",
    "        prev_event = user_event\n",
    "    \n",
    "    training_events.append(training_user_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0669fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TrainingEvent(event=<EventEnum.VIEW: 'view'>, item_category=0, item_property=0, property_value='', season=<SeasonEnum.SPRING: 1>, timedelta_days=0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "training_events[random.randint(0, len(training_events)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.common.enums import EventEnum\n",
    "\n",
    "props: list[int | None] = []\n",
    "values: list[str | None] = []\n",
    "categories: list[int | None] = []\n",
    "\n",
    "for user_training_event in training_events:\n",
    "    for training_event in user_training_event:\n",
    "        if training_event.item_property not in props:\n",
    "            props.append(training_event.item_property)\n",
    "        if training_event.property_value not in values:\n",
    "            values.append(training_event.property_value)\n",
    "        if training_event.item_category not in categories:\n",
    "            categories.append(training_event.item_category)\n",
    "    \n",
    "\n",
    "num_propeties = len(props)\n",
    "num_propet_values = len(values)\n",
    "num_categories = len(categories)\n",
    "num_actions = len(EventEnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.modules.ml.model import prepare_data\n",
    "\n",
    "all_actions: list[list[int]] = []\n",
    "all_properties: list[list[int]] = []\n",
    "all_values: list[list[str]] = []\n",
    "all_categories: list[list[int]] = []\n",
    "all_deltas: list[list[int]] = []\n",
    "all_seasons: list[list[int]] = []\n",
    "\n",
    "all_target_actions: list[list[int]] = []\n",
    "all_target_properties: list[list[int]] = []\n",
    "all_target_values: list[list[str]] = []\n",
    "all_target_categories: list[list[int]] = []\n",
    "for training_event in training_events:\n",
    "    data = prepare_data(training_event)\n",
    "    if data is not None:\n",
    "        (\n",
    "            actions,\n",
    "            properties,\n",
    "            values,\n",
    "            categories,\n",
    "            deltas,\n",
    "            seasons,\n",
    "            target_actions,\n",
    "            target_properties,\n",
    "            target_values,\n",
    "            target_categories\n",
    "        ) = data\n",
    "        all_actions.extend(actions)\n",
    "        all_properties.extend(properties)\n",
    "        all_values.extend(values)\n",
    "        all_categories.extend(categories)\n",
    "        all_deltas.extend(deltas)\n",
    "        all_seasons.extend(seasons)\n",
    "        all_target_actions.extend(target_actions)\n",
    "        all_target_properties.extend(target_properties)\n",
    "        all_target_values.extend(target_values)\n",
    "        all_target_categories.extend(target_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4f5c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import StringLookup\n",
    "\n",
    "# Преобразуем значения в строки (заменяем None на пустую строку)\n",
    "all_values_str = []\n",
    "for values in all_values:\n",
    "    values_str = [str(v) if v is not None else \"\" for v in values]\n",
    "    all_values_str.append(values_str)\n",
    "\n",
    "all_target_values_str = []\n",
    "for target_values in all_target_values:\n",
    "    target_values_str = [str(v) if v is not None else \"\" for v in target_values]\n",
    "    all_target_values_str.append(target_values_str)\n",
    "\n",
    "# Создаем vocabulary, исключая пустую строку (которая будет mask_token)\n",
    "vocab_set = set()\n",
    "for values in all_values_str + all_target_values_str:\n",
    "    for value in values:\n",
    "        if value != \"\":  # Исключаем пустую строку\n",
    "            vocab_set.add(value)\n",
    "\n",
    "vocab = sorted(list(vocab_set))\n",
    "\n",
    "# Создаем lookup слой без указания mask_token в vocabulary\n",
    "lookup_layer = StringLookup(vocabulary=vocab, output_mode=\"int\", mask_token=\"\")\n",
    "\n",
    "# Теперь преобразуем данные\n",
    "vectorized_values = []\n",
    "for values in all_values_str:\n",
    "    # Преобразуем None/пустые строки в маскирующий токен\n",
    "    encoded = lookup_layer(values)\n",
    "    vectorized_values.append(encoded)\n",
    "\n",
    "target_vectorized_values = []\n",
    "for target_values in all_target_values_str:\n",
    "    encoded = lookup_layer(target_values)\n",
    "    target_vectorized_values.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b49adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Преобразуем в numpy массивы\n",
    "all_actions = np.array(all_actions, dtype=np.int32)\n",
    "all_properties = np.array(all_properties, dtype=np.int32)\n",
    "vectorized_values = np.array(vectorized_values, dtype=np.int32)\n",
    "all_categories = np.array(all_categories, dtype=np.int32)\n",
    "all_deltas = np.array(all_deltas, dtype=np.float32)\n",
    "all_seasons = np.array(all_seasons, dtype=np.int32)\n",
    "\n",
    "all_target_actions = np.array(all_target_actions, dtype=np.int32)\n",
    "all_target_properties = np.array(all_target_properties, dtype=np.int32)\n",
    "target_vectorized_values = np.array(target_vectorized_values, dtype=np.int32)\n",
    "all_target_categories = np.array(all_target_categories, dtype=np.int32)\n",
    "\n",
    "# Добавляем размерность для временных дельт\n",
    "all_deltas = np.expand_dims(all_deltas, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdc1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем train_targets и val_targets в one-hot формат\n",
    "def convert_to_onehot(targets_list, num_classes_list):\n",
    "    \"\"\"Преобразует список целочисленных меток в one-hot формат\"\"\"\n",
    "    onehot_list = []\n",
    "    for i, targets in enumerate(targets_list):\n",
    "        # targets: (n_samples, SEQ_LENGTH)\n",
    "        # Нужно преобразовать в (n_samples, SEQ_LENGTH, num_classes)\n",
    "        n_samples, seq_len = targets.shape\n",
    "        num_classes = num_classes_list[i]\n",
    "        \n",
    "        # Преобразуем в one-hot\n",
    "        onehot = np.zeros((n_samples, seq_len, num_classes), dtype=np.float32)\n",
    "        for s in range(n_samples):\n",
    "            for t in range(seq_len):\n",
    "                idx = targets[s, t]\n",
    "                if idx < num_classes:  # Проверка на выход за границы\n",
    "                    onehot[s, t, idx] = 1.0\n",
    "        onehot_list.append(onehot)\n",
    "    return onehot_list\n",
    "\n",
    "# Определяем количество классов для каждого выхода\n",
    "num_classes_list = [\n",
    "    num_actions,        # для action_output\n",
    "    num_propeties,      # для property_output  \n",
    "    num_propet_values,  # для value_output\n",
    "    num_categories      # для category_output\n",
    "]\n",
    "\n",
    "# Преобразуем целевые данные в one-hot\n",
    "train_targets_onehot = convert_to_onehot(train_targets, num_classes_list)\n",
    "val_targets_onehot = convert_to_onehot(val_targets, num_classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88a474",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Преобразуем в one-hot\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_targets[i].shape) == \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     train_onehot_flat = \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_target_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     train_onehot = train_onehot_flat.reshape(n_samples, seq_len, num_classes)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\keras\\src\\utils\\numerical_utils.py:98\u001b[39m, in \u001b[36mto_categorical\u001b[39m\u001b[34m(x, num_classes)\u001b[39m\n\u001b[32m     96\u001b[39m batch_size = x.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     97\u001b[39m categorical = np.zeros((batch_size, num_classes))\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mcategorical\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m = \u001b[32m1\u001b[39m\n\u001b[32m     99\u001b[39m output_shape = input_shape + (num_classes,)\n\u001b[32m    100\u001b[39m categorical = np.reshape(categorical, output_shape)\n",
      "\u001b[31mIndexError\u001b[39m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# После получения train_inputs, val_inputs, train_targets, val_targets\n",
    "# train_inputs имеет форму (6, n_samples, SEQ_LENGTH), но нужно (n_samples, SEQ_LENGTH) для каждого входа\n",
    "\n",
    "# Распаковываем train_inputs\n",
    "train_inputs_unpacked = [\n",
    "    train_inputs[0],  # all_actions\n",
    "    train_inputs[1],  # all_properties\n",
    "    train_inputs[2],  # vectorized_values\n",
    "    train_inputs[3],  # all_categories\n",
    "    train_inputs[4],  # all_deltas (уже с добавленной размерностью)\n",
    "    train_inputs[5],  # all_seasons\n",
    "]\n",
    "\n",
    "val_inputs_unpacked = [\n",
    "    val_inputs[0],  # all_actions\n",
    "    val_inputs[1],  # all_properties\n",
    "    val_inputs[2],  # vectorized_values\n",
    "    val_inputs[3],  # all_categories\n",
    "    val_inputs[4],  # all_deltas\n",
    "    val_inputs[5],  # all_seasons\n",
    "]\n",
    "\n",
    "# Преобразуем train_targets и val_targets в one-hot формат\n",
    "train_targets_onehot = []\n",
    "val_targets_onehot = []\n",
    "\n",
    "num_classes_list = [\n",
    "    num_actions,\n",
    "    num_propeties,\n",
    "    num_propet_values,\n",
    "    num_categories\n",
    "]\n",
    "\n",
    "# Проверяем, что у нас есть данные для обучения\n",
    "if len(train_targets) > 0 and len(train_targets[0]) > 0:\n",
    "    for i in range(4):  # 4 целевых выхода\n",
    "        n_samples = train_targets[i].shape[0]\n",
    "        seq_len = train_targets[i].shape[1] if len(train_targets[i].shape) > 1 else 1\n",
    "        num_classes = num_classes_list[i]\n",
    "        \n",
    "        # Преобразуем в one-hot\n",
    "        if len(train_targets[i].shape) == 2:\n",
    "            train_target_flat = train_targets[i].reshape(-1)\n",
    "            train_onehot_flat = to_categorical(train_target_flat, num_classes=num_classes)\n",
    "            train_onehot = train_onehot_flat.reshape(n_samples, seq_len, num_classes)\n",
    "        else:\n",
    "            train_onehot = to_categorical(train_targets[i], num_classes=num_classes)\n",
    "            \n",
    "        train_targets_onehot.append(train_onehot)\n",
    "        \n",
    "        # Аналогично для val_targets\n",
    "        n_samples_val = val_targets[i].shape[0]\n",
    "        if len(val_targets[i].shape) == 2:\n",
    "            val_target_flat = val_targets[i].reshape(-1)\n",
    "            val_onehot_flat = to_categorical(val_target_flat, num_classes=num_classes)\n",
    "            val_onehot = val_onehot_flat.reshape(n_samples_val, seq_len, num_classes)\n",
    "        else:\n",
    "            val_onehot = to_categorical(val_targets[i], num_classes=num_classes)\n",
    "            \n",
    "        val_targets_onehot.append(val_onehot)\n",
    "\n",
    "# Убедимся, что все массивы имеют правильную форму\n",
    "print(\"Проверка форм данных:\")\n",
    "for i, inp in enumerate(train_inputs_unpacked):\n",
    "    print(f\"train_inputs[{i}].shape = {inp.shape}\")\n",
    "\n",
    "for i, tar in enumerate(train_targets_onehot):\n",
    "    print(f\"train_targets_onehot[{i}].shape = {tar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences = []\n",
    "seq_length = 10\n",
    "\n",
    "for i in range(len(all_actions)):\n",
    "    all_sequences.append(\n",
    "        {\n",
    "            \"actions\": all_actions[i],\n",
    "            \"params\": [int(param) if param != '' else 0 for param in all_properties[i]],\n",
    "            \"values\": [int(val.replace(\" \", \"\")) if val != '' else 0 for val in all_values[i]],\n",
    "            \"categories\": all_categories[i],\n",
    "            \"days_since_prev\": all_deltas[i],\n",
    "            \"seasons\": all_seasons[i],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d52a857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализация модели...\n",
      "Обучение модели...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Обучаем модель\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mОбучение модели...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_on_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Тестируем предсказание\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mТестирование предсказаний...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\app\\modules\\ml\\sasrec_model.py:552\u001b[39m, in \u001b[36mComplexSASRec.train_on_sequences\u001b[39m\u001b[34m(self, train_sequences, val_sequences)\u001b[39m\n\u001b[32m    537\u001b[39m (\n\u001b[32m    538\u001b[39m     actions,\n\u001b[32m    539\u001b[39m     params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    548\u001b[39m     next_category,\n\u001b[32m    549\u001b[39m ) = batch\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdays_since_prev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseasons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Цели (уже в правильном формате: 1...n)\u001b[39;00m\n\u001b[32m    563\u001b[39m targets = {\n\u001b[32m    564\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m: next_action,\n\u001b[32m    565\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m\"\u001b[39m: next_param,\n\u001b[32m    566\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: next_value,\n\u001b[32m    567\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: next_category,\n\u001b[32m    568\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\app\\modules\\ml\\sasrec_model.py:344\u001b[39m, in \u001b[36mComplexSASRec.forward\u001b[39m\u001b[34m(self, actions, params, values, categories, days_since_prev, seasons, sequence_lengths)\u001b[39m\n\u001b[32m    341\u001b[39m batch_size, seq_len = actions.shape\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# Получаем эмбеддинги событий\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m event_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevent_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays_since_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasons\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# Добавляем позиционные эмбеддинги\u001b[39;00m\n\u001b[32m    349\u001b[39m positions = (\n\u001b[32m    350\u001b[39m     torch.arange(seq_len, device=actions.device)\n\u001b[32m    351\u001b[39m     .unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    352\u001b[39m     .expand(batch_size, -\u001b[32m1\u001b[39m)\n\u001b[32m    353\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\app\\modules\\ml\\sasrec_model.py:58\u001b[39m, in \u001b[36mComplexEventEmbedding.forward\u001b[39m\u001b[34m(self, actions, params, values, categories, days_since_prev, seasons)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     49\u001b[39m     actions: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> torch.Tensor:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Получаем эмбеддинги\u001b[39;00m\n\u001b[32m     57\u001b[39m     action_emb = \u001b[38;5;28mself\u001b[39m.action_embedding(actions)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     param_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     value_emb = \u001b[38;5;28mself\u001b[39m.value_embedding(values)\n\u001b[32m     60\u001b[39m     category_emb = \u001b[38;5;28mself\u001b[39m.category_embedding(categories)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from app.modules.ml.sasrec_model import ComplexSASRec\n",
    "# Разделяем на train/val\n",
    "train_size = int(0.8 * len(all_sequences))\n",
    "train_sequences = all_sequences[:train_size]\n",
    "val_sequences = all_sequences[train_size:]\n",
    "\n",
    "# Определяем размеры словарей\n",
    "\n",
    "# Создаем модель\n",
    "print(\"Инициализация модели...\")\n",
    "model = ComplexSASRec(\n",
    "    n_actions=num_actions,\n",
    "    n_params=num_propeties,\n",
    "    n_values=num_propet_values,\n",
    "    n_categories=num_categories,\n",
    "    max_seq_length=11,\n",
    "    embedding_dim=32,\n",
    "    num_blocks=2,\n",
    "    num_heads=2,\n",
    "    dropout_rate=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=16,\n",
    "    num_epochs=5,\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "print(\"Обучение модели...\")\n",
    "model.train_on_sequences(train_sequences, val_sequences)\n",
    "\n",
    "# Тестируем предсказание\n",
    "print(\"\\nТестирование предсказаний...\")\n",
    "test_sequence = {\n",
    "    \"actions\": [1, 2, 3, 1, 2, 3, 1, 2],\n",
    "    \"params\": [1, 3, 2, 1, 3, 2, 1, 3],\n",
    "    \"values\": [5, 10, 7, 5, 10, 7, 5, 10],\n",
    "    \"categories\": [1, 1, 2, 1, 2, 1, 2, 1],\n",
    "    \"days_since_prev\": [0.0, 1.0, 0.5, 2.0, 0.0, 1.5, 0.5, 2.0],\n",
    "    \"seasons\": [1, 1, 2, 2, 3, 3, 4, 4],\n",
    "}\n",
    "\n",
    "prediction = model.predict_next_event(test_sequence)\n",
    "\n",
    "print(\"\\nПредсказание следующего события:\")\n",
    "for key, value in prediction.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143d85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attr 'Toutput_types' of 'OptionalFromValue' Op passed list of length 0 less than minimum 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproperty_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcategory_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproperty_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcategory_output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets_onehot\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\programming\\sofigs-ml\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:153\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperimental\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m            \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    158\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Attr 'Toutput_types' of 'OptionalFromValue' Op passed list of length 0 less than minimum 1."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    {\n",
    "        \"action_output\": train_targets_onehot[0],\n",
    "        \"property_output\": train_targets_onehot[1],\n",
    "        \"value_output\": train_targets_onehot[2],\n",
    "        \"category_output\": train_targets_onehot[3]\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(\n",
    "        val_inputs,\n",
    "        {\n",
    "            \"action_output\": val_targets_onehot[0],\n",
    "            \"property_output\": val_targets_onehot[1],\n",
    "            \"value_output\": val_targets_onehot[2],\n",
    "            \"category_output\": val_targets_onehot[3]\n",
    "        }\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
